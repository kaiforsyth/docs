---
title: SEO
description: Welcome to the home of your new documentation
---

## Robots.txt

A `robots.txt` file tells search engine crawlers which URLs the crawler can access on a website. It also helps search engines to discover the sitemap for a website. This file is automatically generated for every Hall community site.

Your site’s file can be accessed from the `/robots.txt` path. For example, it can be viewed by navigating to `example.hall.community/robots.txt` and replacing that URL with your site’s domain.

## Sitemap generation

We automatically generate an XML sitemap for every community site. This helps search engine crawlers discover and index all of the content and pages on your Hall community site.

It tells search engines like Google when a page was last modified (`<lastmod>`) and how frequently it should check back for updates (`<changefreq>`). You can see the typical shape of a sitemap below:

````
<url>
<loc>https://example.hall.community/how-do-i-change-my-password-qNrvDJFMPCWg</loc>
<lastmod>2024-09-16T06:15:53.703Z</lastmod>
<changefreq>weekly</changefreq>
</url>
````

Your site’s sitemap can be accessed from the `/sitemap.xml` path. For example, it can be viewed by navigating to `example.hall.community/sitemap.xml` and replacing that URL with your site’s domain.

### Submitting sitemap to Google

asdfas

## Structured data

Every Hall community site automatically supports special structured data formats that enables Google to display pages as enriched search results.

Specifically, Hall currently supports the `DiscussionForumPosting` and `QAPage` structured data schemas documented by Google.

https://developers.google.com/search/docs/appearance/structured-data/discussion-forum